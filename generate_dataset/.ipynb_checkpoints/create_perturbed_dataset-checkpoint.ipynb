{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, load_from_disk\n",
    "from transformers import AutoTokenizer\n",
    "import tqdm\n",
    "\n",
    "import spacy\n",
    "import random\n",
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    "spacy = nlp\n",
    "\n",
    "import pickle\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xsum_filtered_train = load_from_disk(\"data/xsum_filtered/train\")\n",
    "# xsum_filtered_val = load_from_disk(\"data/xsum_filtered/val\")\n",
    "# xsum_filtered_test = load_from_disk(\"data/xsum_filtered/test\")\n",
    "xsum_filtered_train._data = xsum_filtered_train._data.filter(xsum_filtered_train['keep'])\n",
    "xsum_corrupted_train = xsum_filtered_train.add_column('corrupted_summary', xsum_filtered_train['summary'])\n",
    "xsum_corrupted_train = xsum_corrupted_train.add_column('corrupted_flag', ['False']* len(xsum_corrupted_train))\n",
    "\n",
    "# xsum_filtered_val._data = xsum_filtered_val._data.filter(xsum_filtered_val['keep'])\n",
    "# xsum_corrupted_val = xsum_filtered_val.add_column('corrupted_summary', xsum_filtered_val['summary'])\n",
    "\n",
    "# xsum_filtered_test._data = xsum_filtered_test._data.filter(xsum_filtered_test['keep'])\n",
    "# xsum_corrupted_test = xsum_filtered_test.add_column('corrupted_summary', xsum_filtered_test['summary'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add noise function\n",
    "\n",
    "NOISE_PROB = 1\n",
    "DELETE_PROB = 0.8\n",
    "def addnoise(summary, augmentation_span):\n",
    "        summary_tokens = [token.text_with_ws for token in summary]\n",
    "\n",
    "        new_summary = []\n",
    "        for ix, token in enumerate(summary_tokens):\n",
    "            # don't modify text inside an augmented span\n",
    "            apply_augmentation = True\n",
    "            if augmentation_span:\n",
    "              for aug_span in augmentation_span:\n",
    "                if aug_span:\n",
    "                  span_start, span_end = aug_span\n",
    "                  if span_start <= ix <= span_end:\n",
    "                      apply_augmentation = False\n",
    "\n",
    "            # decide whether to add noise\n",
    "            if apply_augmentation and random.random() < NOISE_PROB:\n",
    "                # decide whether to replicate or delete token\n",
    "                if random.random() < DELETE_PROB:\n",
    "                    # update spans and skip token\n",
    "                    if augmentation_span:\n",
    "                      for el in range(0, len(augmentation_span)):\n",
    "                        aug_span = augmentation_span[el]\n",
    "                        if aug_span:\n",
    "                          span_start, span_end = aug_span\n",
    "                          if ix < span_start:\n",
    "                            span_start -= 1\n",
    "                            span_end -= 1\n",
    "                          aug_span = span_start, span_end\n",
    "                          augmentation_span[el] = aug_span\n",
    "                      if len(new_summary) > 0:\n",
    "                        if new_summary[-1][-1] != \" \":\n",
    "                          new_summary[-1] = new_summary[-1] + \" \"\n",
    "                      continue      \n",
    "                else:\n",
    "                  if augmentation_span:\n",
    "                    for el in range(0, len(augmentation_span)):\n",
    "                      aug_span = augmentation_span[el]\n",
    "                      if aug_span:\n",
    "                        span_start, span_end = aug_span\n",
    "                        if ix < span_start:\n",
    "                          span_start += 1\n",
    "                          span_end += 1\n",
    "                        aug_span = span_start, span_end\n",
    "                        augmentation_span[el] = aug_span\n",
    "\n",
    "                  new_summary.append(token)\n",
    "            new_summary.append(token)\n",
    "        new_summary = spacy(\"\".join(new_summary))\n",
    "\n",
    "        if summary.text == new_summary.text:\n",
    "#             return new_summary\n",
    "            return new_summary\n",
    "        else:\n",
    "            return new_summary\n",
    "        \n",
    "docs_to_noise = xsum_corrupted_train['id']\n",
    "docs_to_noise = np.random.choice(docs_to_noise, int(0.05*len(docs_to_noise)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def noise_summary(example):\n",
    "    if example['id'] in docs_to_noise:\n",
    "        doc = nlp(example['summary'])\n",
    "        new_doc = addnoise(doc, [(0, len(doc))])\n",
    "        example['corrupted_summary'] = new_doc.text\n",
    "        return example\n",
    "    else:\n",
    "        return example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xsum_corrupted_train = xsum_corrupted_train.map(noise_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comment our perturbations you don't want. We use this to create \n",
    "\n",
    "perturbations = [\n",
    "    \"predicate\",\n",
    "    \"smart\",\n",
    "    \"s_o\", \n",
    "    \"bt\", \n",
    "#     \"prn\", \n",
    "#     \"dat\", \n",
    "#     \"num\", \n",
    "#     \"ent\", \n",
    "#     \"neg\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change num_each_perturbation if you want to add more perturbations to train on.\n",
    "num_each_perturbation = 10000\n",
    "for pert_type in perturbations:\n",
    "    \n",
    "    with open(f'data/corruption_dict/{pert_type}.pkl', 'rb') as f:\n",
    "        corrupted_summaries = pickle.load(f)\n",
    "        \n",
    "    def corrupt_summary(example):\n",
    "        docid = example['id']\n",
    "        if docid in corrupted_summaries:\n",
    "            example['corrupted_summary'] = corrupted_summaries[docid]\n",
    "            example['corrupted_flag'] = pert_type\n",
    "            return example\n",
    "        else:\n",
    "            return example\n",
    "        \n",
    "    filters = list(corrupted_summaries.keys())\n",
    "    \n",
    "    \n",
    "    filters = filters[:num_each_perturbation]\n",
    "    \n",
    "    corrupted_summaries = {ids: corrupted_summaries[ids] for ids in filters}\n",
    "    xsum_corrupted_train = xsum_corrupted_train.map(corrupt_summary)\n",
    "#     xsum_corrupted_train = xsum_corrupted_train.map(corrupt_summary)\n",
    "#     xsum_corrupted_train = xsum_corrupted_train.map(corrupt_summary)\n",
    "\n",
    "    print(f'Added {len(corrupted_summaries)} {pert_type} corruptions to the dataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xsum_corrupted_train.save_to_disk(\"data/xsum_corrupted_predicateonly/train\")\n",
    "# xsum_corrupted_val.save_to_disk(\"data/xsum_corrupted/val\")\n",
    "# xsum_corrupted_test.save_to_disk(\"data/xsum_corrupted/test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entity only perturbations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xsum_filtered_train = load_from_disk(\"data/xsum_filtered/train\")\n",
    "# xsum_filtered_val = load_from_disk(\"data/xsum_filtered/val\")\n",
    "# xsum_filtered_test = load_from_disk(\"data/xsum_filtered/test\")\n",
    "xsum_filtered_train._data = xsum_filtered_train._data.filter(xsum_filtered_train['keep'])\n",
    "xsum_corrupted_train = xsum_filtered_train.add_column('corrupted_summary', xsum_filtered_train['summary'])\n",
    "xsum_corrupted_train = xsum_corrupted_train.add_column('corrupted_flag', ['False']* len(xsum_corrupted_train))\n",
    "\n",
    "# xsum_filtered_val._data = xsum_filtered_val._data.filter(xsum_filtered_val['keep'])\n",
    "# xsum_corrupted_val = xsum_filtered_val.add_column('corrupted_summary', xsum_filtered_val['summary'])\n",
    "\n",
    "# xsum_filtered_test._data = xsum_filtered_test._data.filter(xsum_filtered_test['keep'])\n",
    "# xsum_corrupted_test = xsum_filtered_test.add_column('corrupted_summary', xsum_filtered_test['summary'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comment our perturbations you don't want.\n",
    "\n",
    "perturbations = [\n",
    "#     \"predicate\",\n",
    "#     \"smart\",\n",
    "#     \"s_o\", \n",
    "    \"bt\", \n",
    "    \"prn\", \n",
    "    \"dat\", \n",
    "    \"num\", \n",
    "    \"ent\", \n",
    "    \"neg\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change num_each_perturbation if you want to add more perturbations to train on.\n",
    "num_each_perturbation = 8000\n",
    "for pert_type in perturbations:\n",
    "    \n",
    "    with open(f'data/corruption_dict/{pert_type}.pkl', 'rb') as f:\n",
    "        corrupted_summaries = pickle.load(f)\n",
    "        \n",
    "    def corrupt_summary(example):\n",
    "        docid = example['id']\n",
    "        if docid in corrupted_summaries:\n",
    "            example['corrupted_summary'] = corrupted_summaries[docid]\n",
    "            example['corrupted_flag'] = pert_type\n",
    "            return example\n",
    "        else:\n",
    "            return example\n",
    "        \n",
    "    filters = list(corrupted_summaries.keys())\n",
    "    \n",
    "    \n",
    "    filters = filters[:num_each_perturbation]\n",
    "    \n",
    "    corrupted_summaries = {ids: corrupted_summaries[ids] for ids in filters}\n",
    "    xsum_corrupted_train = xsum_corrupted_train.map(corrupt_summary)\n",
    "\n",
    "    print(f'Added {len(corrupted_summaries)} {pert_type} corruptions to the dataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xsum_corrupted_train.save_to_disk(\"data/xsum_corrupted_entityonly/train\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs282",
   "language": "python",
   "name": "cs282"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
